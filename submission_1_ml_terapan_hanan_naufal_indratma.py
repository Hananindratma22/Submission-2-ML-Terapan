# -*- coding: utf-8 -*-
"""Submission_1_ML_Terapan_Hanan_Naufal_Indratma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mD2pVf9HLES7QUDw_FcXyT2bHQKD92H6

# Identitas Diri
---

**Nama**: Hanan Naufal Indratma  
**Cohort ID**: MC008D5Y2239  
**Group**: MC-10

# **Import Library**

Pada tahap ini, akan diimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier,
    HistGradientBoostingClassifier, AdaBoostClassifier
)
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

"""# **Memuat Dataset**

Memuat dataset hasil clustering dari file CSV ke dalam variabel DataFrame.
"""

# Import Data
url = "https://drive.google.com/uc?export=download&id=15HDlXfQfxhdTX1kFWnuyfAch1kv_ukMQ"
data = pd.read_csv(url)

data

df = data.copy()

df.shape

"""# **Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset. EDA bertujuan untuk:

   - Tinjau jumlah baris dan kolom dalam dataset.  
   - Tinjau jenis data di setiap kolom .
   - Cek duplikasi data dan missing value.
   - Analisis distribusi variabel numerik dengan statistik deskriptif

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.
"""

df.shape

"""Dataset memiliki baris sebanyak 569 dan kolom sebanyak 32."""

df.info()

"""Berdasarkan pengecekan pada tipe data, diperoleh tipe data id adalah integer, diagnosis adalah object, serta variabel lain memiliki tipe data float."""

df.duplicated().sum()

"""Tidak ditemukan data duplikat."""

df.isnull().sum().sum()

"""Tidak ditemukan data hilang/*missing value*."""

df.describe()

"""Data memiliki skala nilai yang berbeda-beda, sehingga akan lebih bagus jika dilakukan *scaling* atau *normalization*.

# **Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang akan dilakukan:
1. Drop Kolom Tidak Diperlukan
2. Encoding Variabel Kategorik
3. Standarisasi Fitur Numerik
"""

# Drop kolom id
df.drop(['id'], axis=1, inplace=True)

"""Kolom id tidak diperlukan pada saat pemodelan, sehingga kolom id akan dibuang dari dataset."""

# Encoding Variabel "diagnosis" menggunakan Label Encoder
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

"""Dilakukan encoding pada variabel diagnosis dengan mengganti nilai M = 1 dan B = 0."""

# Normalisasi Fitur Numerik
from sklearn.preprocessing import MinMaxScaler

# numerical_cols adalah kolom dengan tipe data numerik
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

scaler = MinMaxScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

"""Normalisasi fitur menggunakan MinMaxScaler() dilakukan pada variabel numerik agar data memiliki nilai maksimum dan minimum yang sama.

# **Data Splitting**

Tahap Data Splitting bertujuan untuk memisahkan dataset menjadi dua bagian: data latih (training set) dan data uji (test set).
"""

df.head()

# Splitting Data df
X = df.drop(['diagnosis'], axis=1)
y = df['diagnosis']

# Train Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Data splitting dilakukan dengan perbandingan train dan test sebesar 80 : 20.

# **Pemodelan Klasifikasi**

## **a. Membangun Model Klasifikasi**
"""

# Daftar model
models = {
    "Ridge Classifier": RidgeClassifier(),
    "Support Vector Machine": SVC(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="mlogloss")
}

# Simpan hasil evaluasi
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="macro")  # Ubah ke "macro" untuk multiclass
    precision = precision_score(y_test, y_pred, average="macro")
    recall = recall_score(y_test, y_pred, average="macro")

    results.append([name, accuracy, f1, precision, recall])

"""### Penjelasan Algoritma Klasifikasi

Berikut adalah deskripsi dari algoritma-algoritma klasifikasi yang digunakan:

---

#### 1. Ridge Classifier
Ridge Classifier merupakan versi klasifikasi dari **Ridge Regression**, yang menggunakan regularisasi L2 untuk menghindari overfitting. Cocok digunakan pada data multikolinearitas (fitur yang saling berkorelasi).

---

#### 2. Support Vector Machine (SVM)
SVM bekerja dengan mencari hyperplane terbaik yang memisahkan kelas-kelas dalam data. Cocok untuk data berdimensi tinggi dan bisa digunakan dengan kernel (misalnya RBF) untuk menangani data non-linier.

---

#### 3. Random Forest
Ensemble dari banyak Decision Tree yang dibangun pada subset acak data dan fitur. Mengurangi overfitting dan meningkatkan akurasi dengan teknik **bagging**.

---

#### 4. XGBoost
**Extreme Gradient Boosting**, versi cepat dan efisien dari Gradient Boosting. Mendukung regularisasi, early stopping, dan paralelisme. Sangat populer dalam kompetisi data science karena akurasi tinggi dan fleksibilitas.

## **b. Evaluasi Model Klasifikasi**
"""

# Tampilkan hasil dalam dataframe
df_results = pd.DataFrame(results, columns=["Model", "Accuracy", "F1 Score", "Precision", "Recall"])
df_results.sort_values(by="Accuracy", ascending=False, inplace=True)

print(df_results)

"""- **Support Vector Machine (SVM)** memiliki skor tertinggi untuk semua metrik: akurasi (97.36%), F1 Score (97.18%), precision (97.42%), dan recall (96.97%). Ini menunjukkan bahwa SVM memberikan performa terbaik dalam hal klasifikasi secara keseluruhan.
- **Random Forest** juga menunjukkan performa yang tinggi, namun sedikit di bawah SVM pada setiap metrik.
- **Ridge Classifier dan XGBoost** menunjukkan hasil yang mirip, namun masih kalah dari SVM dan Random Forest.
"""